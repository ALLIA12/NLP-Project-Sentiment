{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-12T08:59:59.641809600Z",
     "start_time": "2024-02-12T08:59:58.035138300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thehi\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: azureml.core: AzureML support for Python 3.7 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.7 workflows that use AzureML will continue to work without modification, but Python 3.7 users will no longer get access to the latest AzureML features and bugfixes. We recommend that you upgrade to Python 3.8 or newer. To disable SDK V1 deprecation warning set the environment variable AZUREML_DEPRECATE_WARNING to 'False'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import collections.abc\n",
    "from azureml.core import Workspace, ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from dotenv import load_dotenv\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350c509a-aa48-47fd-8252-1342e2ece205 TestGroup WorkSpaceUAE-2 uaenorth\n"
     ]
    }
   ],
   "source": [
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can retrieve each variable using os.getenv('VARIABLE_NAME')\n",
    "subscription_id = os.getenv('SUBSCRIPTION_ID')\n",
    "resource_group = os.getenv('RESOURCE_GROUP')\n",
    "workspace_name = os.getenv('WORKSPACE_NAME')\n",
    "region = os.getenv('REGION')\n",
    "\n",
    "# You can now use these variables in your application\n",
    "print(subscription_id, resource_group, workspace_name, region)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:00:00.122402700Z",
     "start_time": "2024-02-12T09:00:00.100321800Z"
    }
   },
   "id": "2ab05fc3df89f893"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying KeyVault with name workspackeyvaultbca5afdd.\n",
      "Deploying AppInsights with name workspacinsights95d3d1b1.\n",
      "Deployed AppInsights with name workspacinsights95d3d1b1. Took 12.26 seconds.\n",
      "Deploying StorageAccount with name workspacstoraged6e2aca3f.\n",
      "Deployed KeyVault with name workspackeyvaultbca5afdd. Took 29.07 seconds.\n",
      "Deployed StorageAccount with name workspacstoraged6e2aca3f. Took 29.83 seconds.\n",
      "Deploying Workspace with name WorkSpaceUAE-2.\n",
      "Deployed Workspace with name WorkSpaceUAE-2. Took 59.48 seconds.\n",
      "Workspace WorkSpaceUAE-2 created\n"
     ]
    }
   ],
   "source": [
    "# Create a workspace\n",
    "\n",
    "ws = Workspace.create(name=workspace_name,\n",
    "                      subscription_id=subscription_id,\n",
    "                      resource_group=resource_group,\n",
    "                      location=region)\n",
    "\n",
    "print(f'Workspace {workspace_name} created')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:02:06.515422Z",
     "start_time": "2024-02-12T09:00:01.840376100Z"
    }
   },
   "id": "dcf6c9d0bdfc5599"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model sentiment_analysis_model\n",
      "Model registered successfully: sentiment_analysis_model:1\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your SavedModel directory\n",
    "model_path = 'NLP-Project-Sentiment/BuiltModel'\n",
    "model_name = 'sentiment_analysis_model'\n",
    "\n",
    "# Register the model in Azure Machine Learning\n",
    "registered_model = Model.register(model_path=model_path,\n",
    "                                  model_name=model_name,\n",
    "                                  workspace=ws)\n",
    "\n",
    "print(\"Model registered successfully:\", registered_model.id)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:02:16.282408900Z",
     "start_time": "2024-02-12T09:02:06.516420500Z"
    }
   },
   "id": "72a4bc169206ad40"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model my_tokenizer\n",
      "Tokenizer model registered successfully: my_tokenizer:1\n"
     ]
    }
   ],
   "source": [
    "# Specify the local file path to the tokenizer\n",
    "tokenizer_file_path = 'NLP-Project-Sentiment/tokenizer.pickle'  # Update this path\n",
    "\n",
    "# Specify a name for the tokenizer model\n",
    "tokenizer_model_name = 'my_tokenizer'\n",
    "\n",
    "# Register the tokenizer as a model in Azure ML\n",
    "registered_tokenizer = Model.register(model_path=tokenizer_file_path,\n",
    "                                      model_name=tokenizer_model_name,\n",
    "                                      workspace=ws)\n",
    "\n",
    "print(\"Tokenizer model registered successfully:\", registered_tokenizer.id)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:02:23.867324300Z",
     "start_time": "2024-02-12T09:02:16.281409500Z"
    }
   },
   "id": "d1b798832113f3cd"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Create a new environment\n",
    "conda_env = Environment('my-tf-conda-env')\n",
    "# Define the conda dependencies object\n",
    "conda_deps = CondaDependencies()\n",
    "# Add conda and pip packages\n",
    "# Note: Specify the versions if necessary to ensure compatibility\n",
    "conda_deps.add_conda_package('numpy')\n",
    "conda_deps.add_pip_package('tensorflow')  # For TensorFlow\n",
    "\n",
    "# Set the dependencies for the environment\n",
    "conda_env.python.conda_dependencies = conda_deps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:02:23.882406Z",
     "start_time": "2024-02-12T09:02:23.869836200Z"
    }
   },
   "id": "7bdd9d6f2803ce67"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Create an InferenceConfig\n",
    "inference_config = InferenceConfig(entry_script='script.py', environment=conda_env)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:49:00.632659500Z",
     "start_time": "2024-02-12T09:49:00.619089Z"
    }
   },
   "id": "2fac7706e8f94978"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "InferenceConfig(entry_script=script.py, runtime=None, conda_file=None, extra_docker_file_steps=None, source_directory=None, enable_gpu=None, base_image=None, base_image_registry=<azureml.core.container_registry.ContainerRegistry object at 0x000001CF02B51848>)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:49:05.729642Z",
     "start_time": "2024-02-12T09:49:05.706630800Z"
    }
   },
   "id": "c993c0ebb1bdb52b"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Specify deployment configuration for ACI\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:12:52.353438800Z",
     "start_time": "2024-02-12T09:12:52.325310200Z"
    }
   },
   "id": "cde0767901d89cbb"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thehi\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2024-02-12 12:49:43+03:00 Creating Container Registry if not exists.\n",
      "2024-02-12 12:49:50+03:00 Use the existing image.\n",
      "2024-02-12 12:49:50+03:00 Generating deployment configuration.\n",
      "2024-02-12 12:49:53+03:00 Submitting deployment to compute..\n",
      "2024-02-12 12:50:02+03:00 Checking the status of deployment sentiment-analysis-service-2..\n",
      "2024-02-12 12:52:17+03:00 Checking the status of inference endpoint sentiment-analysis-service-2.\n",
      "Failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Service deployment polling reached non-successful terminal state, current service state: Failed\n",
      "Operation ID: 02769ceb-3b35-4c96-9bee-6fd1b540ed88\n",
      "More information can be found using '.get_logs()'\n",
      "Error:\n",
      "{\n",
      "  \"code\": \"AciDeploymentFailed\",\n",
      "  \"statusCode\": 400,\n",
      "  \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n",
      "\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n",
      "\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n",
      "\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\",\n",
      "  \"details\": [\n",
      "    {\n",
      "      \"code\": \"CrashLoopBackOff\",\n",
      "      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n",
      "\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n",
      "\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n",
      "\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\"\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"AciDeploymentFailed\",\n",
      "      \"message\": \"Your container application crashed. Please follow the steps to debug:\n",
      "\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\n",
      "\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\n",
      "\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n",
      "\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\n",
      "\"RestartCount\": 3\n",
      "\"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n",
      "\"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2024-02-12T09:54:02.146Z\",\"exitCode\":111,\"finishTime\":\"2024-02-12T09:54:21.597Z\",\"detailStatus\":\"Error\"}\n",
      "\"Events\": null\n",
      "\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "WebserviceException",
     "evalue": "WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 02769ceb-3b35-4c96-9bee-6fd1b540ed88\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"AciDeploymentFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\",\n  \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\"\n    },\n    {\n      \"code\": \"AciDeploymentFailed\",\n      \"message\": \"Your container application crashed. Please follow the steps to debug:\n\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\n\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\n\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\n\"RestartCount\": 3\n\"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n\"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2024-02-12T09:54:02.146Z\",\"exitCode\":111,\"finishTime\":\"2024-02-12T09:54:21.597Z\",\"detailStatus\":\"Error\"}\n\"Events\": null\n\"\n    }\n  ]\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: 02769ceb-3b35-4c96-9bee-6fd1b540ed88\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\"\\n    },\\n    {\\n      \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n      \\\"message\\\": \\\"Your container application crashed. Please follow the steps to debug:\\n\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\\n\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\\n\\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\n\\\"RestartCount\\\": 3\\n\\\"CurrentState\\\": {\\\"state\\\":\\\"Waiting\\\",\\\"startTime\\\":null,\\\"exitCode\\\":null,\\\"finishTime\\\":null,\\\"detailStatus\\\":\\\"CrashLoopBackOff: Back-off restarting failed\\\"}\\n\\\"PreviousState\\\": {\\\"state\\\":\\\"Terminated\\\",\\\"startTime\\\":\\\"2024-02-12T09:54:02.146Z\\\",\\\"exitCode\\\":111,\\\"finishTime\\\":\\\"2024-02-12T09:54:21.597Z\\\",\\\"detailStatus\\\":\\\"Error\\\"}\\n\\\"Events\\\": null\\n\\\"\\n    }\\n  ]\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mWebserviceException\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\3184673015.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m                        \u001B[0minference_config\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minference_config\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# Your inference configuration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m                        deployment_config=aci_config)  # Your deployment configuration\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mservice\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait_for_deployment\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshow_output\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\azureml\\core\\webservice\\webservice.py\u001B[0m in \u001B[0;36mwait_for_deployment\u001B[1;34m(self, show_output, timeout_sec)\u001B[0m\n\u001B[0;32m    918\u001B[0m                                           \u001B[1;34m'Error:\\n'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    919\u001B[0m                                           '{}'.format(self.state, self._operation_endpoint.split('/')[-1],\n\u001B[1;32m--> 920\u001B[1;33m                                                       logs_response, format_error_response), logger=module_logger)\n\u001B[0m\u001B[0;32m    921\u001B[0m             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\n\u001B[0;32m    922\u001B[0m                                                                                   operation_state))\n",
      "\u001B[1;31mWebserviceException\u001B[0m: WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 02769ceb-3b35-4c96-9bee-6fd1b540ed88\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"AciDeploymentFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\",\n  \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\"\n    },\n    {\n      \"code\": \"AciDeploymentFailed\",\n      \"message\": \"Your container application crashed. Please follow the steps to debug:\n\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\n\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\n\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\n\"RestartCount\": 3\n\"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n\"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2024-02-12T09:54:02.146Z\",\"exitCode\":111,\"finishTime\":\"2024-02-12T09:54:21.597Z\",\"detailStatus\":\"Error\"}\n\"Events\": null\n\"\n    }\n  ]\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: 02769ceb-3b35-4c96-9bee-6fd1b540ed88\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: sentiment-analysis-service-2. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image 2b6cb3cd62e94ddb8955eb988071702a.azurecr.io/azureml/azureml_36aa3adc3ab2df19e10bc80943b571d8 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\"\\n    },\\n    {\\n      \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n      \\\"message\\\": \\\"Your container application crashed. Please follow the steps to debug:\\n\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\\n\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\\n\\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\n\\\"RestartCount\\\": 3\\n\\\"CurrentState\\\": {\\\"state\\\":\\\"Waiting\\\",\\\"startTime\\\":null,\\\"exitCode\\\":null,\\\"finishTime\\\":null,\\\"detailStatus\\\":\\\"CrashLoopBackOff: Back-off restarting failed\\\"}\\n\\\"PreviousState\\\": {\\\"state\\\":\\\"Terminated\\\",\\\"startTime\\\":\\\"2024-02-12T09:54:02.146Z\\\",\\\"exitCode\\\":111,\\\"finishTime\\\":\\\"2024-02-12T09:54:21.597Z\\\",\\\"detailStatus\\\":\\\"Error\\\"}\\n\\\"Events\\\": null\\n\\\"\\n    }\\n  ]\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "# Assuming 'registered_model' is your main model and 'registered_tokenizer' is your tokenizer model\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name='sentiment-analysis-service-2',  # Name of the deployment\n",
    "                       models=[registered_model, registered_tokenizer],  # Include both models\n",
    "                       inference_config=inference_config,  # Your inference configuration\n",
    "                       deployment_config=aci_config)  # Your deployment configuration\n",
    "service.wait_for_deployment(show_output=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:54:44.312517900Z",
     "start_time": "2024-02-12T09:49:35.050912Z"
    }
   },
   "id": "c61b92ef880b3115"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'/bin/bash: /azureml-envs/azureml_fa0a7495e1f695c796ee93bc9880efec/lib/libtinfo.so.6: no version information available (required by /bin/bash)\\n/bin/bash: /azureml-envs/azureml_fa0a7495e1f695c796ee93bc9880efec/lib/libtinfo.so.6: no version information available (required by /bin/bash)\\n/bin/bash: /azureml-envs/azureml_fa0a7495e1f695c796ee93bc9880efec/lib/libtinfo.so.6: no version information available (required by /bin/bash)\\n2024-02-12T09:47:29,833583150+00:00 - rsyslog/run \\nbash: /azureml-envs/azureml_fa0a7495e1f695c796ee93bc9880efec/lib/libtinfo.so.6: no version information available (required by bash)\\n2024-02-12T09:47:29,840080347+00:00 - gunicorn/run \\n2024-02-12T09:47:29,845341207+00:00 | gunicorn/run | \\n2024-02-12T09:47:29,846751149+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:29,848137091+00:00 | gunicorn/run | AzureML Container Runtime Information\\n2024-02-12T09:47:29,856936258+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:29,862134715+00:00 - nginx/run \\n2024-02-12T09:47:29,863490356+00:00 | gunicorn/run | \\n2024-02-12T09:47:29,870118657+00:00 | gunicorn/run | \\n2024-02-12T09:47:29,879021927+00:00 | gunicorn/run | AzureML image information: openmpi4.1.0-ubuntu20.04, Materializaton Build:20240123.v2\\n2024-02-12T09:47:29,880496172+00:00 | gunicorn/run | \\n2024-02-12T09:47:29,882802842+00:00 | gunicorn/run | \\n2024-02-12T09:47:29,888332909+00:00 | gunicorn/run | PATH environment variable: /azureml-envs/azureml_fa0a7495e1f695c796ee93bc9880efec/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\n2024-02-12T09:47:29,892692241+00:00 | gunicorn/run | PYTHONPATH environment variable: \\n2024-02-12T09:47:29,895976641+00:00 | gunicorn/run | \\n2024-02-12T09:47:32,749343474+00:00 | gunicorn/run | CONDAPATH environment variable: /opt/miniconda\\n\\n# conda environments:\\n#\\n                      *  /azureml-envs/azureml_fa0a7495e1f695c796ee93bc9880efec\\nbase                     /opt/miniconda\\n\\n2024-02-12T09:47:34,172387564+00:00 | gunicorn/run | \\n2024-02-12T09:47:34,176363682+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\\n\\nabsl-py==2.1.0\\nadal==1.2.7\\nargcomplete==3.2.2\\nastunparse==1.6.3\\nattrs==23.2.0\\nazure-common==1.1.28\\nazure-core==1.29.1\\nazure-graphrbac==0.61.1\\nazure-identity==1.15.0\\nazure-mgmt-authorization==4.0.0\\nazure-mgmt-containerregistry==10.3.0\\nazure-mgmt-core==1.4.0\\nazure-mgmt-keyvault==10.3.0\\nazure-mgmt-network==25.2.0\\nazure-mgmt-resource==23.0.1\\nazure-mgmt-storage==21.1.0\\nazureml-core==1.55.0\\nazureml-dataprep==5.1.4\\nazureml-dataprep-native==41.0.0\\nazureml-dataprep-rslex==2.22.2\\nazureml-dataset-runtime==1.55.0\\nazureml-defaults==1.55.0\\nazureml-inference-server-http==1.0.0\\nbackports.tempfile==1.0\\nbackports.weakref==1.0.post1\\nbcrypt==4.1.2\\ncachetools==5.3.2\\ncertifi==2024.2.2\\ncffi==1.16.0\\ncharset-normalizer==3.3.2\\nclick==8.1.7\\ncloudpickle==2.2.1\\ncontextlib2==21.6.0\\ncryptography==41.0.7\\ndocker==7.0.0\\nFlask==2.2.5\\nFlask-Cors==3.0.10\\nflatbuffers==23.5.26\\nfusepy==3.0.1\\ngast==0.4.0\\ngoogle-api-core==2.17.0\\ngoogle-auth==2.27.0\\ngoogle-auth-oauthlib==1.0.0\\ngoogle-pasta==0.2.0\\ngoogleapis-common-protos==1.62.0\\ngrpcio==1.60.1\\ngunicorn==20.1.0\\nh5py==3.10.0\\nhumanfriendly==10.0\\nidna==3.6\\nimportlib-metadata==7.0.1\\nimportlib-resources==6.1.1\\ninference-schema==1.7.1\\nisodate==0.6.1\\nitsdangerous==2.1.2\\njeepney==0.8.0\\nJinja2==3.1.3\\njmespath==1.0.1\\njsonpickle==3.0.2\\njsonschema==4.21.1\\njsonschema-specifications==2023.12.1\\nkeras==2.13.1\\nknack==0.11.0\\nlibclang==16.0.6\\nMarkdown==3.5.2\\nMarkupSafe==2.1.5\\nmkl-fft @ file:///croot/mkl_fft_1695058164594/work\\nmkl-random @ file:///croot/mkl_random_1695059800811/work\\nmkl-service==2.4.0\\nmsal==1.26.0\\nmsal-extensions==1.1.0\\nmsrest==0.7.1\\nmsrestazure==0.6.4\\nndg-httpsclient==0.5.1\\nnumpy==1.23.5\\noauthlib==3.2.2\\nopencensus==0.11.4\\nopencensus-context==0.1.3\\nopencensus-ext-azure==1.1.13\\nopt-einsum==3.3.0\\npackaging==23.2\\nparamiko==3.4.0\\npathspec==0.12.1\\npkginfo==1.9.6\\npkgutil_resolve_name==1.3.10\\nportalocker==2.8.2\\nprotobuf==4.25.2\\npsutil==5.9.8\\npyarrow==15.0.0\\npyasn1==0.5.1\\npyasn1-modules==0.3.0\\npycparser==2.21\\npydantic==1.10.14\\nPygments==2.17.2\\nPyJWT==2.8.0\\nPyNaCl==1.5.0\\npyOpenSSL==23.3.0\\nPySocks==1.7.1\\npython-dateutil==2.8.2\\npytz==2024.1\\nPyYAML==6.0.1\\nreferencing==0.33.0\\nrequests==2.31.0\\nrequests-oauthlib==1.3.1\\nrpds-py==0.17.1\\nrsa==4.9\\nSecretStorage==3.3.3\\nsix==1.16.0\\ntabulate==0.9.0\\ntensorboard==2.13.0\\ntensorboard-data-server==0.7.2\\ntensorflow==2.13.1\\ntensorflow-estimator==2.13.0\\ntensorflow-io-gcs-filesystem==0.34.0\\ntermcolor==2.4.0\\ntyping_extensions==4.5.0\\nurllib3==2.2.0\\nWerkzeug==3.0.1\\nwrapt==1.16.0\\nzipp==3.17.0\\n\\n2024-02-12T09:47:35,380628986+00:00 | gunicorn/run | \\n2024-02-12T09:47:35,386781768+00:00 | gunicorn/run | Entry script directory: /var/azureml-app/.\\n2024-02-12T09:47:35,388491319+00:00 | gunicorn/run | \\n2024-02-12T09:47:35,390299172+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:35,395520127+00:00 | gunicorn/run | Dynamic Python Package Installation\\n2024-02-12T09:47:35,397457685+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:35,399129734+00:00 | gunicorn/run | \\n2024-02-12T09:47:35,405089211+00:00 | gunicorn/run | Dynamic Python package installation is disabled.\\n2024-02-12T09:47:35,406869364+00:00 | gunicorn/run | \\n2024-02-12T09:47:35,408879123+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:35,411294395+00:00 | gunicorn/run | Checking if the Python package azureml-inference-server-http is installed\\n2024-02-12T09:47:35,416202640+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:35,417772087+00:00 | gunicorn/run | \\n2024-02-12T09:47:36,995824773+00:00 | gunicorn/run | \\n2024-02-12T09:47:37,002618974+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:37,007050406+00:00 | gunicorn/run | AzureML Inference Server\\n2024-02-12T09:47:37,009417376+00:00 | gunicorn/run | ###############################################\\n2024-02-12T09:47:37,012646472+00:00 | gunicorn/run | \\n2024-02-12T09:47:37,014717433+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\\n2024-02-12 09:47:37,370 I [647] azmlinfsrv - Loaded logging config from /azureml-envs/azureml_fa0a7495e1f695c796ee93bc9880efec/lib/python3.8/site-packages/azureml_inference_server_http/logging.json\\n2024-02-12 09:47:37,518 I [647] gunicorn.error - Starting gunicorn 20.1.0\\n2024-02-12 09:47:37,520 I [647] gunicorn.error - Listening at: http://0.0.0.0:31311 (647)\\n2024-02-12 09:47:37,520 I [647] gunicorn.error - Using worker: sync\\n2024-02-12 09:47:37,525 I [709] gunicorn.error - Booting worker with pid: 709\\n\\nAzure ML Inferencing HTTP server v1.0.0\\n\\n\\nServer Settings\\n---------------\\nEntry Script Name: /var/azureml-app/script.py\\nModel Directory: /var/azureml-app/azureml-models\\nConfig File: None\\nWorker Count: 1\\nWorker Timeout (seconds): 300\\nServer Port: 31311\\nHealth Port: 31311\\nApplication Insights Enabled: false\\nApplication Insights Key: None\\nInferencing HTTP server version: azmlinfsrv/1.0.0\\nCORS for the specified origins: None\\nCreate dedicated endpoint for health: None\\n\\n\\nServer Routes\\n---------------\\nLiveness Probe: GET   127.0.0.1:31311/\\nScore:          POST  127.0.0.1:31311/score\\n\\n2024-02-12 09:47:38,285 I [709] azmlinfsrv - AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\\nInitializing logger\\n2024-02-12 09:47:38,289 I [709] azmlinfsrv - Starting up app insights client\\n2024-02-12 09:47:39.285343: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\\n2024-02-12 09:47:39.813495: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\\n2024-02-12 09:47:39.817374: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.get_logs()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T09:47:44.015960600Z",
     "start_time": "2024-02-12T09:47:39.858611700Z"
    }
   },
   "id": "a7a0e0674bca02a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scoring_uri = service.scoring_uri\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9d9af13cb3b0ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scoring_uri\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9456e4859c5c43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
